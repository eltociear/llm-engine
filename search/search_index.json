{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u26a1 LLM Engine \u26a1","text":"<p>The open source engine for fine-tuning large language models. LLM Engine is the easiest way to customize and serve LLMs. Use Scale's hosted version or run it in your own cloud.</p>"},{"location":"#quick-install","title":"\ud83d\udcbb Quick Install","text":"Install using pip <pre><code>pip install scale-llm-engine\n</code></pre>"},{"location":"#about","title":"\ud83e\udd14 About","text":"<p>Foundation models are emerging as the building blocks of AI. However, deploying these models to the cloud and fine-tuning them still requires infrastructure and ML expertise, and can be expensive.</p> <p>LLM Engine is a Python library, CLI, and Helm chart that provides everything you need to fine-tune and serve foundation models in the cloud using Kubernetes. Key features include:</p> <p>\ud83d\ude80 Ready-to-use Fine-Tuning and Inference APIs for your favorite models: LLM Engine comes with ready-to-use APIs for your favorite open-source models, including MPT, Falcon, and LLaMA. Use Scale-hosted endpoints or deploy to your own infrastructure.</p> <p>\ud83d\udc33 Deploying from any docker image: Turn any Docker image into an auto-scaling deployment with simple APIs.</p> <p>\ud83c\udf99\ufe0fOptimized Inference: LLM Engine provides inference APIs for streaming responses and dynamically batching inputs for higher throughput and lower latency.</p> <p>\ud83e\udd17 Open-Source Integrations: Deploy any Huggingface model with a single command.</p>"},{"location":"#features-coming-soon","title":"\ud83d\udd25 Features Coming Soon","text":"<p>\u2744 Fast Cold-Start Times: To prevent GPUs from idling, LLM Engine automatically scales your model to zero when it's not in use and scales up within seconds, even for large foundation models.</p> <p>\ud83d\udcb8 Cost-Optimized: Deploy AI models cheaper than commercial ones, including cold-start and warm-down times.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"getting_started/","title":"\ud83d\ude80 Getting Started","text":"<p>To start using LLM Engine's public inference and fine-tuning APIs:</p> Install using pipInstall using conda <pre><code>pip install scale-llm-engine\n</code></pre> <pre><code>conda install scale-llm-engine -c conda-forge\n</code></pre> <p>Navigate to https://spellbook.scale.com where you will get a Scale API key on the settings page. Set this API key as the <code>SCALE_API_KEY</code> environment variable by adding the following line to your <code>.zshrc</code> or <code>.bash_profile</code>:</p> Set API key <pre><code>export SCALE_API_KEY = \"[Your API key]\"\n</code></pre> <p>With your API key set, you can now send LLM Engine requests using the Python client:</p> Using the Python Client <pre><code>from llmengine import Completion\nresponse = Completion.create(\nmodel_name=\"llama-7b\",\nprompt=\"Hello, my name is\",\nmax_new_tokens=10,\ntemperature=0.2,\n)\nprint(response.outputs[0].text)\n</code></pre>"},{"location":"model_zoo/","title":"\ud83e\udd99 Public Model Zoo","text":"<p>Scale hosts the following models in a model zoo:</p> Model Name Inference APIs Available Fine-tuning APIs Available <code>llama-7b</code> \u2705 \u2705 <code>falcon-7b</code> \u2705 <code>falcon-7b-instruct</code> \u2705 <code>falcon-40b</code> \u2705 <code>falcon-40b-instruct</code> \u2705 <code>mpt-7b</code> \u2705 <code>mpt-7b-instruct</code> \u2705 \u2705 <code>flan-t5-xxl</code> \u2705 <p>Each of these models can be used with the Completion API.</p>"},{"location":"api/error_handling/","title":"Error handling","text":"<p>LLM Engine uses conventional HTTP response codes to indicate the success or failure of an API request. In general: codes in the <code>2xx</code> range indicate success. Codes in the <code>4xx</code> range indicate indicate an error that failed given the  information provided (e.g. a given Model was not found, or an invalid temperature was specified). Codes in the <code>5xx</code>  range indicate an error with the LLM Engine servers.</p> <p>In the Python client, errors are presented via a set of corresponding Exception classes, which should be caught  and handled by the user accordingly.</p>"},{"location":"api/error_handling/#llmengine.errors.BadRequestError","title":"BadRequestError","text":"<pre><code>BadRequestError(message: str)\n</code></pre> <p>         Bases: <code>Exception</code></p> <p>Corresponds to HTTP 400. Indicates that the request had inputs that were invalid. The user should not attempt to retry the request without changing the inputs.</p>"},{"location":"api/error_handling/#llmengine.errors.UnauthorizedError","title":"UnauthorizedError","text":"<pre><code>UnauthorizedError(message: str)\n</code></pre> <p>         Bases: <code>Exception</code></p> <p>Corresponds to HTTP 401. This means that no valid API key was provided.</p>"},{"location":"api/error_handling/#llmengine.errors.NotFoundError","title":"NotFoundError","text":"<pre><code>NotFoundError(message: str)\n</code></pre> <p>         Bases: <code>Exception</code></p> <p>Corresponds to HTTP 404. This means that the resource (e.g. a Model, FineTune, etc.) could not be found. Note that this can also be returned in some cases where the object might exist, but the user does not have access to the object. This is done to avoid leaking information about the existence or nonexistence of said object that the user does not have access to.</p>"},{"location":"api/error_handling/#llmengine.errors.RateLimitExceededError","title":"RateLimitExceededError","text":"<pre><code>RateLimitExceededError(message: str)\n</code></pre> <p>         Bases: <code>Exception</code></p> <p>Corresponds to HTTP 429. Too many requests hit the API too quickly. We recommend an exponential backoff for retries.</p>"},{"location":"api/error_handling/#llmengine.errors.ServerError","title":"ServerError","text":"<pre><code>ServerError(status_code: int, message: str)\n</code></pre> <p>         Bases: <code>Exception</code></p> <p>Corresponds to HTTP 5xx errors on the server.</p>"},{"location":"api/langchain/","title":"\ud83e\udd9c Langchain","text":"<p>Coming soon!</p>"},{"location":"api/python_client/","title":"\ud83d\udc0d Python Client API Reference","text":""},{"location":"api/python_client/#llmengine.Completion","title":"Completion","text":"<p>         Bases: <code>APIEngine</code></p> <p>Completion API. This API is used to generate text completions. The Completions API can be run either synchronous or asynchronously (via Python <code>asyncio</code>); for each of these modes, you can also choose to stream token responses or not.</p>"},{"location":"api/python_client/#llmengine.completion.Completion.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(model_name: str, prompt: str, max_new_tokens: int = 20, temperature: float = 0.2, timeout: int = 10, stream: bool = False) -&gt; Union[CompletionSyncV1Response, Iterator[CompletionStreamV1Response]]\n</code></pre> <p>Creates a completion for the provided prompt and parameters synchronously.</p> Example request without token streaming <pre><code>from llmengine import Completion\nresponse = Completion.create(\nmodel_name=\"llama-7b\",\nprompt=\"Hello, my name is\",\nmax_new_tokens=10,\ntemperature=0.2,\n)\nprint(response.json())\n</code></pre> JSON Response <pre><code>{\n\"status\": \"SUCCESS\",\n\"outputs\":\n[\n{\n\"text\": \"_______ and I am a _______\",\n\"num_prompt_tokens\": null,\n\"num_completion_tokens\": 10\n}\n],\n\"traceback\": null\n}\n</code></pre> Example request with token streaming <pre><code>from llmengine import Completion\nstream = Completion.create(\nmodel_name=\"llama-7b\",\nprompt=\"why is the sky blue?\",\nmax_new_tokens=5,\ntemperature=0.2,\nstream=True,\n)\nfor response in stream:\nif response.output:\nprint(response.json())\n</code></pre> JSON responses <pre><code>{\"status\": \"SUCCESS\", \"output\": {\"text\": \"\\n\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 1 }, \"traceback\": null }\n{\"status\": \"SUCCESS\", \"output\": {\"text\": \"I\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 2 }, \"traceback\": null }\n{\"status\": \"SUCCESS\", \"output\": {\"text\": \" don\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 3 }, \"traceback\": null }\n{\"status\": \"SUCCESS\", \"output\": {\"text\": \"\u2019\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 4 }, \"traceback\": null }\n{\"status\": \"SUCCESS\", \"output\": {\"text\": \"t\", \"finished\": true, \"num_prompt_tokens\": null, \"num_completion_tokens\": 5 }, \"traceback\": null }\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to use. See Model Zoo for a list of Models that are supported.</p> required <code>prompt</code> <code>str</code> <p>The prompt to generate completions for, encoded as a string.</p> required <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the completion.</p> <p>The token count of your prompt plus <code>max_new_tokens</code> cannot exceed the model's context length. See  Model Zoo for information on each supported model's context length.</p> <code>20</code> <code>temperature</code> <code>float</code> <p>What sampling temperature to use, in the range <code>(0, 1]</code>. Higher values like 0.8 will make the output  more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>0.2</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds. This is the maximum amount of time you are willing to wait for a response.</p> <code>10</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. If true, the return type is an <code>Iterator[CompletionStreamV1Response]</code>. Otherwise, the return type is a <code>CompletionSyncV1Response</code>. When streaming, tokens will be sent as data-only server-sent events.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>response</code> <code>Union[CompletionSyncV1Response, AsyncIterable[CompletionStreamV1Response]]</code> <p>The generated response (if <code>streaming=False</code>) or iterator of response chunks (if <code>streaming=True</code>)</p>"},{"location":"api/python_client/#llmengine.completion.Completion.acreate","title":"acreate  <code>async</code> <code>classmethod</code>","text":"<pre><code>acreate(model_name: str, prompt: str, max_new_tokens: int = 20, temperature: float = 0.2, timeout: int = 10, stream: bool = False) -&gt; Union[CompletionSyncV1Response, AsyncIterable[CompletionStreamV1Response]]\n</code></pre> <p>Creates a completion for the provided prompt and parameters asynchronously (with <code>asyncio</code>).</p> Example without token streaming <pre><code>import asyncio\nfrom llmengine import Completion\nasync def main():\nresponse = await Completion.acreate(\nmodel_name=\"llama-7b\",\nprompt=\"Hello, my name is\",\nmax_new_tokens=10,\ntemperature=0.2,\n)\nprint(response.json())\nasyncio.run(main())\n</code></pre> JSON response <pre><code>{\n\"status\": \"SUCCESS\",\n\"outputs\":\n[\n{\n\"text\": \"_______, and I am a _____\",\n\"num_prompt_tokens\": null,\n\"num_completion_tokens\": 10\n}\n],\n\"traceback\": null\n}\n</code></pre> Example with token streaming <pre><code>import asyncio\nfrom llmengine import Completion\nasync def main():\nstream = await Completion.acreate(\nmodel_name=\"llama-7b\",\nprompt=\"why is the sky blue?\",\nmax_new_tokens=5,\ntemperature=0.2,\nstream=True,\n)\nasync for response in stream:\nif response.output:\nprint(response.json())\nasyncio.run(main())\n</code></pre> JSON responses <pre><code>{\"status\": \"SUCCESS\", \"output\": {\"text\": \"\\n\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 1}, \"traceback\": null}\n{\"status\": \"SUCCESS\", \"output\": {\"text\": \"I\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 2}, \"traceback\": null}\n{\"status\": \"SUCCESS\", \"output\": {\"text\": \" think\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 3}, \"traceback\": null}\n{\"status\": \"SUCCESS\", \"output\": {\"text\": \" the\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 4}, \"traceback\": null}\n{\"status\": \"SUCCESS\", \"output\": {\"text\": \" sky\", \"finished\": true, \"num_prompt_tokens\": null, \"num_completion_tokens\": 5}, \"traceback\": null}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to use. See Model Zoo for a list of Models that are supported.</p> required <code>prompt</code> <code>str</code> <p>The prompt to generate completions for, encoded as a string.</p> required <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the completion.</p> <p>The token count of your prompt plus <code>max_new_tokens</code> cannot exceed the model's context length. See  Model Zoo for information on each supported model's context length.</p> <code>20</code> <code>temperature</code> <code>float</code> <p>What sampling temperature to use, in the range <code>(0, 1]</code>. Higher values like 0.8 will make the output  more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>0.2</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds. This is the maximum amount of time you are willing to wait for a response.</p> <code>10</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. If true, the return type is an <code>Iterator[CompletionStreamV1Response]</code>. Otherwise, the return type is a <code>CompletionSyncV1Response</code>. When streaming, tokens will be sent as data-only server-sent events.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>response</code> <code>Union[CompletionSyncV1Response, AsyncIterable[CompletionStreamV1Response]]</code> <p>The generated response (if <code>streaming=False</code>) or iterator of response chunks (if <code>streaming=True</code>)</p>"},{"location":"api/python_client/#llmengine.CompletionOutput","title":"CompletionOutput","text":"<p>         Bases: <code>BaseModel</code></p> <p>Represents the output of a completion request to a model.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionOutput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text of the completion.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionOutput.num_prompt_tokens","title":"num_prompt_tokens  <code>instance-attribute</code>","text":"<pre><code>num_prompt_tokens: Optional[int]\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionOutput.num_completion_tokens","title":"num_completion_tokens  <code>instance-attribute</code>","text":"<pre><code>num_completion_tokens: int\n</code></pre> <p>Number of tokens in the completion.</p>"},{"location":"api/python_client/#llmengine.CompletionSyncV1Response","title":"CompletionSyncV1Response","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for a synchronous prompt completion.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionSyncV1Response.outputs","title":"outputs  <code>instance-attribute</code>","text":"<pre><code>outputs: List[CompletionOutput]\n</code></pre> <p>List of completion outputs.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionSyncV1Response.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: TaskStatus\n</code></pre> <p>Task status.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionSyncV1Response.traceback","title":"traceback  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>traceback: Optional[str] = None\n</code></pre> <p>Traceback if the task failed.</p>"},{"location":"api/python_client/#llmengine.CompletionStreamOutput","title":"CompletionStreamOutput","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamOutput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text of the completion.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamOutput.finished","title":"finished  <code>instance-attribute</code>","text":"<pre><code>finished: bool\n</code></pre> <p>Whether the completion is finished.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamOutput.num_prompt_tokens","title":"num_prompt_tokens  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>num_prompt_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamOutput.num_completion_tokens","title":"num_completion_tokens  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>num_completion_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the completion.</p>"},{"location":"api/python_client/#llmengine.CompletionStreamV1Response","title":"CompletionStreamV1Response","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for a stream prompt completion task.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamV1Response.output","title":"output  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>output: Optional[CompletionStreamOutput] = None\n</code></pre> <p>Completion output.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamV1Response.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: TaskStatus\n</code></pre> <p>Task status.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamV1Response.traceback","title":"traceback  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>traceback: Optional[str] = None\n</code></pre> <p>Traceback if the task failed.</p>"},{"location":"api/python_client/#llmengine.FineTune","title":"FineTune","text":"<p>         Bases: <code>APIEngine</code></p> <p>FineTune API. This API is used to fine-tune models.</p>"},{"location":"api/python_client/#llmengine.fine_tuning.FineTune.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(model: str, training_file: str, validation_file: Optional[str] = None, hyperparameters: Optional[Dict[str, str]] = None, suffix: Optional[str] = None) -&gt; CreateFineTuneResponse\n</code></pre> <p>Creates a job that fine-tunes a specified model from a given dataset.</p> Example <pre><code>from llmengine import FineTune\nresponse = FineTune.create(\nmodel=\"llama-7b\",\ntraining_file=\"s3://my-bucket/path/to/training-file.csv\",\n)\nprint(response.json())\n</code></pre> JSON Response <pre><code>{\n\"fine_tune_id\": \"ft_abc123\"\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>`str`</code> <p>The name of the base model to fine-tune. See #model_zoo for the list of available models to fine-tune.</p> required <code>training_file</code> <code>`str`</code> <p>Path to file of training dataset</p> required <code>validation_file</code> <code>`Optional[str]`</code> <p>Path to file of validation dataset</p> <code>None</code> <code>hyperparameters</code> <code>`str`</code> <p>Hyperparameters</p> <code>None</code> <code>suffix</code> <code>`Optional[str]`</code> <p>A string that will be added to your fine-tuned model name.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CreateFineTuneResponse</code> <code>CreateFineTuneResponse</code> <p>an object that contains the ID of the created fine-tuning job</p>"},{"location":"api/python_client/#llmengine.fine_tuning.FineTune.list","title":"list  <code>classmethod</code>","text":"<pre><code>list() -&gt; ListFineTunesResponse\n</code></pre> <p>List fine-tuning jobs</p> Example <pre><code>from llmengine import FineTune\nresponse = FineTune.list()\nprint(response.json())\n</code></pre> JSON Response <pre><code>[\n{\n\"fine_tune_id\": \"ft_abc123\",\n\"status\": \"RUNNING\"\n},\n{\n\"fine_tune_id\": \"ft_def456\",\n\"status\": \"SUCCESS\"\n}\n]\n</code></pre> <p>Returns:</p> Name Type Description <code>ListFineTunesResponse</code> <code>ListFineTunesResponse</code> <p>an object that contains a list of all fine-tuning jobs and their statuses</p>"},{"location":"api/python_client/#llmengine.fine_tuning.FineTune.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(fine_tune_id: str) -&gt; GetFineTuneResponse\n</code></pre> <p>Get status of a fine-tuning job</p> Example <pre><code>from llmengine import FineTune\nresponse = FineTune.retrieve(\nfine_tune_id=\"ft_abc123\",\n)\nprint(response.json())\n</code></pre> JSON Response <pre><code>{\n\"fine_tune_id\": \"ft_abc123\",\n\"status\": \"RUNNING\"\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>GetFineTuneResponse</code> <code>GetFineTuneResponse</code> <p>an object that contains the ID and status of the requested job</p>"},{"location":"api/python_client/#llmengine.fine_tuning.FineTune.cancel","title":"cancel  <code>classmethod</code>","text":"<pre><code>cancel(fine_tune_id: str) -&gt; CancelFineTuneResponse\n</code></pre> <p>Cancel a fine-tuning job</p> Example <pre><code>from llmengine import FineTune\nresponse = FineTune.cancel(fine_tune_id=\"ft_abc123\")\nprint(response.json())\n</code></pre> JSON Response <pre><code>{\n\"success\": \"true\"\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>CancelFineTuneResponse</code> <code>CancelFineTuneResponse</code> <p>an object that contains whether the cancellation was successful</p>"},{"location":"api/python_client/#llmengine.CreateFineTuneResponse","title":"CreateFineTuneResponse","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for creating a FineTune.</p>"},{"location":"api/python_client/#llmengine.data_types.CreateFineTuneResponse.fine_tune_id","title":"fine_tune_id  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>fine_tune_id: str = Field(Ellipsis, description='ID of the created fine-tuning job.')\n</code></pre> <p>The ID of the FineTune.</p>"},{"location":"api/python_client/#llmengine.GetFineTuneResponse","title":"GetFineTuneResponse","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for retrieving a FineTune.</p>"},{"location":"api/python_client/#llmengine.data_types.GetFineTuneResponse.fine_tune_id","title":"fine_tune_id  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>fine_tune_id: str = Field(Ellipsis, description='ID of the requested job.')\n</code></pre> <p>The ID of the FineTune.</p>"},{"location":"api/python_client/#llmengine.data_types.GetFineTuneResponse.status","title":"status  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>status: BatchJobStatus = Field(Ellipsis, description='Status of the requested job.')\n</code></pre> <p>The status of the FineTune job.</p>"},{"location":"api/python_client/#llmengine.ListFineTunesResponse","title":"ListFineTunesResponse","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for listing FineTunes.</p>"},{"location":"api/python_client/#llmengine.data_types.ListFineTunesResponse.jobs","title":"jobs  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>jobs: List[GetFineTuneResponse] = Field(Ellipsis, description='List of fine-tuning jobs and their statuses.')\n</code></pre> <p>A list of FineTunes, represented as <code>GetFineTuneResponse</code>s.</p>"},{"location":"api/python_client/#llmengine.CancelFineTuneResponse","title":"CancelFineTuneResponse","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for cancelling a FineTune.</p>"},{"location":"api/python_client/#llmengine.data_types.CancelFineTuneResponse.success","title":"success  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>success: bool = Field(Ellipsis, description='Whether cancellation was successful.')\n</code></pre> <p>Whether the cancellation succeeded.</p>"},{"location":"guides/completions/","title":"Completions","text":"<p>LLM Engine provides access to open source language models (see Model Zoo) that can be used for producing  Completions.  An example API call looks as follows: <pre><code>from llmengine import Completion\nresponse = Completion.create(\nmodel_name=\"llama-7b\",\nprompt=\"Hello, my name is\",\nmax_new_tokens=10,\ntemperature=0.2,\n)\n</code></pre></p> <p>See the full API reference documentation to learn more.</p>"},{"location":"guides/completions/#completions-response-format","title":"Completions response format","text":"<p>An example Completions API response looks as follows:</p> <pre><code>{\n\"outputs\":\n[\n{\n\"text\": \"_______ and I am a _______\",\n\"num_completion_tokens\": 10\n}\n],\n}\n</code></pre> <p>In Python, the response is of type CompletionSyncV1Response,  which maps to the above JSON structure.</p>"},{"location":"guides/completions/#token-streaming","title":"Token streaming","text":"<p>The Completions API support token streaming to reduce perceived latency for certain applications. When streaming,  tokens will be sent as data-only server-side events.</p> <p>To enable token streaming, pass <code>stream=True</code> to either <code>Completion.create</code> or <code>Completion.acreate</code>.</p> <p>An example of token streaming using the synchronous Completions API looks as follows: <pre><code>from llmengine import Completion\nstream = Completion.create(\nmodel_name=\"llama-7b\",\nprompt=\"why is the sky blue?\",\nmax_new_tokens=5,\ntemperature=0.2,\nstream=True,\n)\nfor response in stream:\nif response.output:\nprint(response.json())\n</code></pre></p>"},{"location":"guides/completions/#async-requests","title":"Async requests","text":"<p>The Python client supports <code>asyncio</code> for creating Completions. Use <code>Completion.acreate</code> instead of <code>Completion.create</code>  to utilize async processing. The function signatures are otherwise identical.</p> <p>An example of async Completions looks as follows: <pre><code>import asyncio\nfrom llmengine import Completion\nasync def main():\nresponse = await Completion.acreate(\nmodel_name=\"llama-7b\",\nprompt=\"Hello, my name is\",\nmax_new_tokens=10,\ntemperature=0.2,\n)\nprint(response.json())\nasyncio.run(main())\n</code></pre></p>"},{"location":"guides/completions/#which-model-should-i-use","title":"Which model should I use?","text":"<p>See the Model Zoo for more information on best practices for which model to use for Completions.</p>"},{"location":"guides/fine_tuning/","title":"Fine Tuning API","text":"<p>The Fine Tuning API allows you to fine tune various open source LLMs on your own data, then make inference calls to the resulting LLM. For more specific details, see the python client docs.</p>"},{"location":"guides/fine_tuning/#preparing-data","title":"Preparing Data","text":"<p>Your data must be formatted as a CSV file that includes two columns: <code>prompt</code> and <code>response</code>. The data needs to be uploaded to somewhere publicly accessible, so that we can read the data to fine tune on it. For example, you can upload your data to a public s3 bucket.</p>"},{"location":"guides/fine_tuning/#launching-the-fine-tune","title":"Launching the Fine Tune","text":"<p>Once you have uploaded your data, you can use our API to launch a Fine Tune. You will need to provide the base model to train off of, the locations of the training and validation files, an optional set of hyperparameters to override, and an optional suffix to append to the name of the fine tune. </p> <p>See the Model Zoo to see which models have fine tuning support.</p> <p>Once the fine tune is launched, you can also get the status of your fine tune.</p>"},{"location":"guides/fine_tuning/#making-inference-calls-to-your-fine-tune","title":"Making inference calls to your fine tune","text":"<p>Once the fine tune is finished, you will be able to start making requests to the model. First, you can list the available LLMs in order to get the name of your fine tuned model. See the completions API for more details. You can then use that name to direct your inference requests.</p>"},{"location":"guides/rate_limits/","title":"Overview","text":""},{"location":"guides/rate_limits/#what-are-rate-limits","title":"What are rate limits?","text":"<p>A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within  a specified period of time.</p>"},{"location":"guides/rate_limits/#why-do-we-have-rate-limits","title":"Why do we have rate limits?","text":"<p>Rate limits are a common practice for APIs, and they're put in place for a few different reasons:</p> <ul> <li>They help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with  requests in an attempt to overload it or cause disruptions in the service. By setting rate limits, the LLM Engine  server can prevent this kind of activity.</li> <li>Rate limits help ensure that everyone has fair access to API. If one person or organization makes an excessive  number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single  user can make, LLM Engine ensures that the most number of people have an opportunity to use the API without  experiencing slowdowns. This also applies when self-hosting LLM Engine, as all internal users within an organization  would have fair access.</li> <li>Rate limits can help manage the aggregate load on the server infrastructure. If requests to the API increase  dramatically, it could tax the servers and cause performance issues. By setting rate limits, LLM Engine can help  maintain a smooth and consistent experience for all users. This is especially important when self-hosting LLM Engine.</li> </ul>"},{"location":"guides/rate_limits/#how-do-i-know-if-i-am-rate-limited","title":"How do I know if I am rate limited?","text":"<p>Per standard HTTP practices, your request will receive a response with HTTP status code of <code>429</code>, <code>Too Many Requests</code>.</p>"},{"location":"guides/rate_limits/#what-are-the-rate-limits-for-our-api","title":"What are the rate limits for our API?","text":"<p>The LLM Engine API is currently in a preview mode, and therefore we currently do not have any advertised rate limits. As the API moves towards a production release, we will update this section with specific rate limits. For now, the API will return HTTP 429 on an as-needed basis.</p>"},{"location":"guides/rate_limits/#error-mitigation","title":"Error mitigation","text":""},{"location":"guides/rate_limits/#retrying-with-exponential-backoff","title":"Retrying with exponential backoff","text":"<p>One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff.  Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the  unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated.  This continues until the request is successful or until a maximum number of retries is reached. This approach has many benefits:</p> <ul> <li>Automatic retries means you can recover from rate limit errors without crashes or missing data</li> <li>Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail</li> <li>Adding random jitter to the delay helps retries from all hitting at the same time.</li> </ul> <p>Below are a few example solutions for Python that use exponential backoff.</p>"},{"location":"guides/rate_limits/#example-1-using-the-tenacity-library","title":"Example #1: Using the <code>tenacity</code> library","text":"<p>Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding  retry behavior to just about anything. To add exponential backoff to your requests, you can use the tenacity.retry  decorator. The below example uses the tenacity.wait_random_exponential function to add random exponential backoff to a  request.</p> <pre><code>import llmengine\nfrom tenacity import (\nretry,\nstop_after_attempt,\nwait_random_exponential,\n)  # for exponential backoff\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\nreturn llmengine.Completion.create(**kwargs)\ncompletion_with_backoff(model_name=\"llama-7b\", prompt=\"Why is the sky blue?\")\n</code></pre>"},{"location":"guides/rate_limits/#example-2-using-the-backoff-library","title":"Example #2: Using the <code>backoff</code> library","text":"<p>Another python library that provides function decorators for backoff and retry is backoff:</p> <pre><code>import llmengine\nimport backoff\n@backoff.on_exception(backoff.expo, llmengine.error.RateLimitError)\ndef completions_with_backoff(**kwargs):\nreturn llmengine.Completion.create(**kwargs)\ncompletions_with_backoff(model_name=\"llama-7b\", prompt=\"Why is the sky blue?\")\n</code></pre>"},{"location":"guides/token_streaming/","title":"Token streaming","text":"<p>The Completions APIs support a <code>stream</code> boolean parameter that, when <code>True</code>, will return a streamed response of token-by-token server-sent events (SSEs) rather than waiting to receive the full response when model generation has finished. This decreases latency of when you start getting a response.</p> <p>The response will consist of SSEs of the form <code>{\"token\": dict, \"generated_text\": str | null, \"details\": dict | null}</code>, where the dictionary for each token will contain log probability information in addition to the generated string; the <code>generated_text</code> field will be <code>null</code> for all but the last SSE, for which it will contain the full generated response.</p>"}]}