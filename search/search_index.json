{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u26a1 LLM Engine \u26a1","text":"<p>The open source engine for fine-tuning large language models. LLM Engine is the easiest way to customize and serve LLMs. Use Scale's hosted version or run it in your own cloud.</p>"},{"location":"#quick-install","title":"\ud83d\udcbb Quick Install","text":"Install using pip <pre><code>pip install scale-llm-engine\n</code></pre>"},{"location":"#about","title":"\ud83e\udd14 About","text":"<p>Foundation models are emerging as the building blocks of AI. However, deploying these models to the cloud and fine-tuning them still requires infrastructure and ML expertise, and can be expensive.</p> <p>LLM Engine is a Python library, CLI, and Helm chart that provides everything you need to fine-tune and serve foundation models in the cloud using Kubernetes. Key features include:</p> <p>\ud83d\ude80 Ready-to-use Fine-Tuning and Inference APIs for your favorite models: LLM Engine comes with ready-to-use APIs for your favorite open-source models, including MPT, Falcon, and LLaMA. Use Scale-hosted endpoints or deploy to your own infrastructure.</p> <p>\ud83d\udc33 Deploying from any docker image: Turn any Docker image into an auto-scaling deployment with simple APIs.</p> <p>\ud83c\udf99\ufe0fOptimized Inference: LLM Engine provides inference APIs for streaming responses and dynamically batching inputs for higher throughput and lower latency.</p> <p>\ud83e\udd17 Open-Source Integrations: Deploy any Huggingface model with a single command.</p>"},{"location":"#features-coming-soon","title":"\ud83d\udd25 Features Coming Soon","text":"<p>\u2744 Fast Cold-Start Times: To prevent GPUs from idling, LLM Engine automatically scales your model to zero when it's not in use and scales up within seconds, even for large foundation models.</p> <p>\ud83d\udcb8 Cost-Optimized: Deploy AI models cheaper than commercial ones, including cold-start and warm-down times.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"getting_started/","title":"\ud83d\ude80 Getting Started","text":"<p>To start using LLM Engine's public inference and fine-tuning APIs:</p> Install using pipInstall using conda <pre><code>pip install scale-llm-engine\n</code></pre> <pre><code>conda install scale-llm-engine -c conda-forge\n</code></pre> <p>Navigate to https://spellbook.scale.com where you will get a Scale API key on the settings page. Set this API key as the <code>SCALE_API_KEY</code> environment variable by adding the following line to your <code>.zshrc</code> or <code>.bash_profile</code>:</p> Set API key <pre><code>export SCALE_API_KEY = \"[Your API key]\"\n</code></pre> <p>With your API key set, you can now send LLM Engine requests using the Python client:</p> Using the Python Client <pre><code>from llmengine import Completion\nresponse = Completion.create(\nmodel_name=\"llama-7b\",\nprompt=\"Hello, my name is\",\nmax_new_tokens=10,\ntemperature=0.2,\n)\nprint(response.outputs[0].text)\n</code></pre>"},{"location":"model_zoo/","title":"\ud83e\udd99 Public Model Zoo","text":"<p>Scale hosts the following models in a model zoo:</p> Model Name Inference APIs Available Fine-tuning APIs Available <code>llama-7b</code> \u2705 \u2705 <code>falcon-7b</code> \u2705 <code>falcon-7b-instruct</code> \u2705 <code>falcon-40b</code> \u2705 <code>falcon-40b-instruct</code> \u2705 <code>mpt-7b</code> \u2705 \u2705 <code>mpt-7b-instruct</code> \u2705 <code>flan-t5-xxl</code> \u2705 <p>Each of these models can be used with the Completion API.</p>"},{"location":"api/langchain/","title":"\ud83e\udd9c Langchain","text":"<p>Coming soon!</p>"},{"location":"api/python_client/","title":"\ud83d\udc0d Python Client API Reference","text":""},{"location":"api/python_client/#llmengine.Completion","title":"Completion","text":"<p>         Bases: <code>APIEngine</code></p> <p>Completion API. This API is used to generate text completions. The Completions API can be run either synchronous or asynchronously (via Python <code>asyncio</code>); for each of these modes, you can also choose to stream token responses or not.</p>"},{"location":"api/python_client/#llmengine.completion.Completion.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\nmodel_name: str,\nprompt: str,\nmax_new_tokens: int = 20,\ntemperature: float = 0.2,\ntimeout: int = 10,\nstream: bool = False,\n) -&gt; Union[\nCompletionSyncV1Response,\nIterator[CompletionStreamV1Response],\n]\n</code></pre> <p>Create a completion task synchronously.</p> Example request without token streaming <pre><code>from llmengine import Completion\nresponse = Completion.create(\nmodel_name=\"llama-7b\",\nprompt=\"Hello, my name is\",\nmax_new_tokens=10,\ntemperature=0.2,\n)\nprint(response)\n</code></pre> JSON Response <pre><code>{\n\"status\": \"SUCCESS\",\n\"outputs\": [\n{\n\"text\": \"\\nThe sky is blue because of the way the light is reflected off the molecules in the air.\\nWhat is the sky blue?\\nThe sky is blue because of the way the light is reflected off the molecules in the air.\\nWhat is the sky blue?\\nThe sky is blue because of the way the light is reflected off the molecules in the air. The sky is blue because of the way the light is reflected off the molecules in the air.\\nWhat is\",\n\"num_prompt_tokens\": null,\n\"num_completion_tokens\": 100\n}\n],\n\"traceback\": null\n}\n</code></pre> Example request with token streaming <pre><code>from llmengine import Completion\nstream = Completion.create(\nmodel_name=\"llama-7b\",\nprompt=\"why is the sky blue?\",\nmax_new_tokens=5,\ntemperature=0.2,\nstream=True,\n)\nfor response in stream:\nif response.output:\nprint(response.output)\n</code></pre> Stream response JSONs <pre><code>[\n{\"text\": \"\\n\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 1},\n{\"text\": \"I\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 2},\n{\"text\": \"'\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 3},\n{\"text\": \"m\", \"finished\": false, \"num_prompt_tokens\": null, \"num_completion_tokens\": 4},\n{\"text\": \" not\", \"finished\": true, \"num_prompt_tokens\": null, \"num_completion_tokens\": 5}\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name to use for inference</p> required <code>prompt</code> <code>str</code> <p>Input text</p> required <code>max_new_tokens</code> <code>int</code> <p>Maximum number of generated tokens</p> <code>20</code> <code>temperature</code> <code>float</code> <p>The value used to module the logits distribution.</p> <code>0.2</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds</p> <code>10</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. If true, the return type is an <code>Iterator</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>response</code> <code>Union[CompletionSyncV1Response, Iterator[CompletionStreamV1Response]]</code> <p>generated response or iterator of response chunks</p>"},{"location":"api/python_client/#llmengine.completion.Completion.acreate","title":"acreate  <code>classmethod</code> <code>async</code>","text":"<pre><code>acreate(\nmodel_name: str,\nprompt: str,\nmax_new_tokens: int = 20,\ntemperature: float = 0.2,\ntimeout: int = 10,\nstream: bool = False,\n) -&gt; Union[\nCompletionSyncV1Response,\nAsyncIterable[CompletionStreamV1Response],\n]\n</code></pre> <p>Create a completion task asynchronously.</p> Example without token streaming <pre><code>from llmengine import Completion\nasync def main():\nresponse_stream = await Completion.acreate(\nmodel_name=\"llama-7b\",\nprompt=\"Hello, my name is\",\nmax_new_tokens=10,\ntemperature=0.2,\n)\nasync for response in response_stream:\nprint(response.output.text)\n</code></pre> <p>JSON Response: <pre><code>\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name to use for inference</p> required <code>prompt</code> <code>str</code> <p>Input text</p> required <code>max_new_tokens</code> <code>int</code> <p>Maximum number of generated tokens</p> <code>20</code> <code>temperature</code> <code>float</code> <p>The value used to module the logits distribution.</p> <code>0.2</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds</p> <code>10</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. If true, the return type is an <code>Iterator[CompletionStreamV1Response]</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>response</code> <code>Union[CompletionSyncV1Response, AsyncIterable[CompletionStreamV1Response]]</code> <p>generated response or iterator of response chunks</p>"},{"location":"api/python_client/#llmengine.FineTune","title":"FineTune","text":"<p>         Bases: <code>APIEngine</code></p> <p>FineTune API. This API is used to fine-tune models.</p>"},{"location":"api/python_client/#llmengine.fine_tuning.FineTune.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\ntraining_file: str,\nvalidation_file: str,\nmodel_name: str,\nbase_model: str,\nfine_tuning_method: str,\nhyperparameters: Dict[str, str],\n) -&gt; CreateFineTuneJobResponse\n</code></pre> <p>Create a fine-tuning job.</p> Example <pre><code>from llmengine import FineTune\nresponse = FineTune.create(\ntraining_file=\"s3://my-bucket/path/to/training-file.csv\",\nvalidation_file=\"s3://my-bucket/path/to/validation-file.csv\",\nmodel_name=\"llama-7b-ft-2023-07-18\",\nbase_model=\"llama-7b\",\nfine_tuning_method=\"ia3\",\nhyperparameters={},\n)\nprint(response)\n</code></pre> JSON Response <pre><code>\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>training_file</code> <code>`str`</code> <p>Path to file of training dataset</p> required <code>validation_file</code> <code>`str`</code> <p>Path to file of validation dataset</p> required <code>model_name</code> <code>`str`</code> <p>Name of the fine-tuned model</p> required <code>base_model</code> <code>`str`</code> <p>Base model to train from</p> required <code>fine_tuning_method</code> <code>`str`</code> <p>Fine-tuning method</p> required <code>hyperparameters</code> <code>`str`</code> <p>Hyperparameters</p> required <p>Returns:</p> Name Type Description <code>CreateFineTuneJobResponse</code> <code>CreateFineTuneJobResponse</code> <p>ID of the created fine-tuning job</p>"},{"location":"api/python_client/#llmengine.fine_tuning.FineTune.list","title":"list  <code>classmethod</code>","text":"<pre><code>list() -&gt; ListFineTuneJobResponse\n</code></pre> <p>List fine-tuning jobs</p> Example <pre><code>from llmengine import FineTune\nresponse = FineTune.list()\nprint(response)\n</code></pre> JSON Response <pre><code>\n</code></pre> <p>Returns:</p> Name Type Description <code>ListFineTuneJobResponse</code> <code>ListFineTuneJobResponse</code> <p>list of all fine-tuning jobs and their statuses</p>"},{"location":"api/python_client/#llmengine.fine_tuning.FineTune.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(fine_tune_id: str) -&gt; GetFineTuneJobResponse\n</code></pre> <p>Get status of a fine-tuning job</p> Example <pre><code>from llmengine import FineTune\nresponse = FineTune.retrieve(\nfine_tune_id=\"ft_abc123...\",\n)\nprint(response)\n</code></pre> JSON Response <pre><code>\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>GetFineTuneJobResponse</code> <code>GetFineTuneJobResponse</code> <p>ID and status of the requested job</p>"},{"location":"api/python_client/#llmengine.fine_tuning.FineTune.cancel","title":"cancel  <code>classmethod</code>","text":"<pre><code>cancel(fine_tune_id: str) -&gt; CancelFineTuneJobResponse\n</code></pre> <p>Cancel a fine-tuning job</p> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>CancelFineTuneJobResponse</code> <code>CancelFineTuneJobResponse</code> <p>whether the cancellation was successful</p>"},{"location":"api/python_client/#llmengine.Model","title":"Model","text":"<p>         Bases: <code>APIEngine</code></p> <p>Model API. This API is used to retrieve, list, and create models.</p> Example <pre><code>from llmengine import Model\nresponse = Model.list()\nprint(response)\n</code></pre>"},{"location":"api/python_client/#llmengine.model.Model.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(model_name: str) -&gt; CreateLLMModelEndpointV1Response\n</code></pre> <p>Create a Model Endpoint. Note: This feature is only available for self-hosted users.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>`str`</code> <p>Name of the model</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>CreateLLMModelEndpointV1Response</code> <p>ID of the created Model Endpoint.</p>"},{"location":"api/python_client/#llmengine.model.Model.list","title":"list  <code>classmethod</code>","text":"<pre><code>list() -&gt; ListLLMModelEndpointsV1Response\n</code></pre> <p>List model endpoints</p> <p>Returns:</p> Name Type Description <code>response</code> <code>ListLLMModelEndpointsV1Response</code> <p>list of model endpoints</p>"},{"location":"api/python_client/#llmengine.model.Model.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(model_name: str) -&gt; GetLLMModelEndpointV1Response\n</code></pre> <p>Get an LLM model endpoint</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>`str`</code> <p>Name of the model</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>GetLLMModelEndpointV1Response</code> <p>object representing the LLM endpoint and configurations</p>"},{"location":"api/python_client/#llmengine.CompletionSyncV1Response","title":"CompletionSyncV1Response","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for a synchronous prompt completion task.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionSyncV1Response.outputs","title":"outputs  <code>instance-attribute</code>","text":"<pre><code>outputs: List[CompletionOutput]\n</code></pre> <p>List of completion outputs.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionSyncV1Response.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: TaskStatus\n</code></pre> <p>Task status.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionSyncV1Response.traceback","title":"traceback  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>traceback: Optional[str] = None\n</code></pre> <p>Traceback if the task failed.</p>"},{"location":"api/python_client/#llmengine.CompletionStreamV1Response","title":"CompletionStreamV1Response","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for a stream prompt completion task.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamV1Response.output","title":"output  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>output: Optional[CompletionStreamOutput] = None\n</code></pre> <p>Completion output.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamV1Response.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: TaskStatus\n</code></pre> <p>Task status.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamV1Response.traceback","title":"traceback  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>traceback: Optional[str] = None\n</code></pre> <p>Traceback if the task failed.</p>"},{"location":"api/python_client/#llmengine.CompletionOutput","title":"CompletionOutput","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/python_client/#llmengine.data_types.CompletionOutput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>Text</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionOutput.num_prompt_tokens","title":"num_prompt_tokens  <code>instance-attribute</code>","text":"<pre><code>num_prompt_tokens: Optional[int]\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionOutput.num_completion_tokens","title":"num_completion_tokens  <code>instance-attribute</code>","text":"<pre><code>num_completion_tokens: int\n</code></pre> <p>Number of tokens in the completion.</p>"},{"location":"api/python_client/#llmengine.CompletionStreamOutput","title":"CompletionStreamOutput","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamOutput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>Text</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamOutput.finished","title":"finished  <code>instance-attribute</code>","text":"<pre><code>finished: bool\n</code></pre> <p>Whether the completion is finished.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamOutput.num_prompt_tokens","title":"num_prompt_tokens  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>num_prompt_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"api/python_client/#llmengine.data_types.CompletionStreamOutput.num_completion_tokens","title":"num_completion_tokens  <code>instance-attribute</code> <code>class-attribute</code>","text":"<pre><code>num_completion_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the completion.</p>"},{"location":"api/python_client/#llmengine.TaskStatus","title":"TaskStatus","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"api/python_client/#llmengine.CreateFineTuneJobRequest","title":"CreateFineTuneJobRequest","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/python_client/#llmengine.data_types.CreateFineTuneJobRequest.base_model","title":"base_model  <code>instance-attribute</code>","text":"<pre><code>base_model: str\n</code></pre> <p>Base model to train from</p>"},{"location":"api/python_client/#llmengine.data_types.CreateFineTuneJobRequest.training_file","title":"training_file  <code>instance-attribute</code>","text":"<pre><code>training_file: str\n</code></pre> <p>Path to file of training dataset</p>"},{"location":"api/python_client/#llmengine.data_types.CreateFineTuneJobRequest.validation_file","title":"validation_file  <code>instance-attribute</code>","text":"<pre><code>validation_file: str\n</code></pre> <p>Path to file of validation dataset</p>"},{"location":"api/python_client/#llmengine.data_types.CreateFineTuneJobRequest.fine_tuning_method","title":"fine_tuning_method  <code>instance-attribute</code>","text":"<pre><code>fine_tuning_method: str\n</code></pre> <p>Fine-tuning method</p>"},{"location":"api/python_client/#llmengine.data_types.CreateFineTuneJobRequest.hyperparameters","title":"hyperparameters  <code>instance-attribute</code>","text":"<pre><code>hyperparameters: Dict[str, str]\n</code></pre> <p>Hyperparameters</p>"},{"location":"api/python_client/#llmengine.data_types.CreateFineTuneJobRequest.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Name of the fine-tuned model</p>"},{"location":"api/python_client/#llmengine.CreateFineTuneJobResponse","title":"CreateFineTuneJobResponse","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/python_client/#llmengine.data_types.CreateFineTuneJobResponse.fine_tune_id","title":"fine_tune_id  <code>instance-attribute</code>","text":"<pre><code>fine_tune_id: str\n</code></pre> <p>ID of the created fine-tuning job</p>"},{"location":"api/python_client/#llmengine.GetFineTuneJobResponse","title":"GetFineTuneJobResponse","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/python_client/#llmengine.data_types.GetFineTuneJobResponse.fine_tune_id","title":"fine_tune_id  <code>instance-attribute</code>","text":"<pre><code>fine_tune_id: str\n</code></pre> <p>ID of the requested job</p>"},{"location":"api/python_client/#llmengine.data_types.GetFineTuneJobResponse.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: BatchJobStatus\n</code></pre> <p>Status of the requested job</p>"},{"location":"api/python_client/#llmengine.ListFineTuneJobResponse","title":"ListFineTuneJobResponse","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/python_client/#llmengine.data_types.ListFineTuneJobResponse.jobs","title":"jobs  <code>instance-attribute</code>","text":"<pre><code>jobs: List[GetFineTuneJobResponse]\n</code></pre> <p>List of fine-tuning jobs and their statuses</p>"},{"location":"api/python_client/#llmengine.CancelFineTuneJobResponse","title":"CancelFineTuneJobResponse","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/python_client/#llmengine.data_types.CancelFineTuneJobResponse.success","title":"success  <code>instance-attribute</code>","text":"<pre><code>success: bool\n</code></pre> <p>Whether cancellation was successful</p>"},{"location":"guides/completions/","title":"Completions","text":"<p>Given a prompt, the model will return one or more predicted Completions. The Completions API can be run either synchronous or asynchronously (via Python <code>asyncio</code>); for each of these modes, you can also choose to stream token responses or not.</p>"},{"location":"guides/token_streaming/","title":"Token streaming","text":"<p>The Completions APIs support a <code>stream</code> boolean parameter that, when <code>True</code>, will return a streamed response of token-by-token server-sent events (SSEs) rather than waiting to receive the full response when model generation has finished. This decreases latency of when you start getting a response.</p> <p>The response will consist of SSEs of the form <code>{\"token\": dict, \"generated_text\": str | null, \"details\": dict | null}</code>, where the dictionary for each token will contain log probability information in addition to the generated string; the <code>generated_text</code> field will be <code>null</code> for all but the last SSE, for which it will contain the full generated response.</p>"}]}